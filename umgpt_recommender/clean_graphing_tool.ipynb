{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing \n",
    "stop_words = set(stopwords.words('english'))\n",
    "exclude = set([\"course\", 'students'])\n",
    "\n",
    "# Remove stopwords and make text lowercase.\n",
    "def clean_description(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    cleaned_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "    cleaned_tokens = [word for word in cleaned_tokens if word.lower() not in exclude and word.isalpha()]\n",
    "    return ' '.join(cleaned_tokens).str.lower()\n",
    "\n",
    "#Function to vectorize description beased on previous basis\n",
    "def vectorize_description(text):\n",
    "    tokens = text.split()\n",
    "    vector = [1 if word in tokens else 0 for word in top_words]\n",
    "    return vector\n",
    "\n",
    "def vectorize_description_scaled(text):\n",
    "    word_counts = Counter(text.split())\n",
    "    # Create a vector where each element is the frequency of a word in the basis_of_words\n",
    "    vector = [word_counts[word] if word in word_counts else 0 for word in top_words]\n",
    "    return vector\n",
    "\n",
    "# Takes in two course names, the name of the column that holds the names, the dataframe, and the similarity matrix\n",
    "# and returns their cosine similarity\n",
    "def get_similarity(name1, name2, type, df, matrix):\n",
    "    idx_1 = df[df[type] == name1].index\n",
    "    idx_1 = idx_1[0]\n",
    "    idx_2 = df[df[type] == name2].index\n",
    "    idx_2 = idx_2[0]\n",
    "    return matrix[idx_1, idx_2]\n",
    "\n",
    "# Gets the course's cleaned description that is used to vectorize it\n",
    "def get_clean_desc(df, name):\n",
    "    idx_1 = df[df['department'] == name].index\n",
    "    idx_1 = idx_1[0]\n",
    "    return df.iloc[idx_1]['cleaned_description']\n",
    "\n",
    "# Gets the course's regular description that is used to vectorize it\n",
    "def get_reg_desc(df, name):\n",
    "    idx_1 = df[df['course'] == name].index\n",
    "    idx_1 = idx_1[0]\n",
    "    return df.iloc[idx_1]['description_x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of loading some course descriptions and making a dictionry to generate vector respesentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = pd.read_pickle('lsa_courses.pkl')\n",
    "eng = pd.read_pickle('eng_courses.pkl')\n",
    "\n",
    "lsa['cleaned_description'] = lsa['description_x'].apply(clean_description)\n",
    "eng['cleaned_description'] = eng['description_x'].apply(clean_description)\n",
    "lsa['cleaned_description'] = lsa['cleaned_description'].str.lower()\n",
    "eng['cleaned_description'] = eng['cleaned_description'].str.lower()\n",
    "\n",
    "\n",
    "#This gets the top 500 most common words in descriptions acrossLSA and ENG\n",
    "combined_df = pd.concat([lsa, eng], ignore_index=True)\n",
    "all_words = ' '.join(combined_df['cleaned_description']).split()\n",
    "word_freq = Counter(all_words)\n",
    "top_words = [word for word, freq in word_freq.most_common(500)]\n",
    "\n",
    "# Generates vector representation of departments by simply concatenating all the course descripions.\n",
    "concatenated_descriptions = combined_df.groupby('department')['description_x'].apply(' '.join).reset_index()\n",
    "concatenated_descriptions['cleaned_description'] = concatenated_descriptions['description_x'].apply(clean_description)\n",
    "\n",
    "all_words = ' '.join(concatenated_descriptions['cleaned_description']).split()\n",
    "word_freq = Counter(all_words)\n",
    "top_words = [word for word, freq in word_freq.most_common(500)]\n",
    "# Generate words\n",
    "concatenated_descriptions['vector'] = concatenated_descriptions['cleaned_description'].apply(vectorize_description_scaled)\n",
    "\n",
    "# Make similarity matrix\n",
    "matrix = [vec for vec in concatenated_descriptions['vector']]\n",
    "# Compute cosine similarities\n",
    "similarity_matrix = cosine_similarity(matrix)\n",
    "#Find similar courses\n",
    "deps = concatenated_descriptions['department'].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
