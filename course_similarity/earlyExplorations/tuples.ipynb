{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "\n",
    "import spacy\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = pd.read_pickle('lsa_courses.pkl')\n",
    "eng = pd.read_pickle('eng_courses.pkl')\n",
    "\n",
    "#Removing \n",
    "stop_words = set(stopwords.words('english'))\n",
    "exclude = set([\"course\", 'students'])\n",
    "\n",
    "def clean_description(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    cleaned_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "    cleaned_tokens = [word for word in cleaned_tokens if word.lower() not in exclude and word.isalpha()]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "lsa['cleaned_description'] = lsa['description_x'].apply(clean_description)\n",
    "eng['cleaned_description'] = eng['description_x'].apply(clean_description)\n",
    "lsa['cleaned_description'] = lsa['cleaned_description'].str.lower()\n",
    "eng['cleaned_description'] = eng['cleaned_description'].str.lower()\n",
    "\n",
    "# Originally used 100 most common, behavior became much 'better' when increased to 500\n",
    "\n",
    "#This gets the top n most common words in descriptions across LSA and ENG\n",
    "combined_df = pd.concat([lsa, eng], ignore_index=True)\n",
    "all_words = ' '.join(combined_df['cleaned_description']).split()\n",
    "word_freq = Counter(all_words)\n",
    "top_words = [word for word, freq in word_freq.most_common(500)]\n",
    "\n",
    "\n",
    "#Function to vectorize description beased on previous basis\n",
    "def vectorize_description(text):\n",
    "    tokens = text.split()\n",
    "    vector = [1 if word in tokens else 0 for word in top_words]\n",
    "    return vector\n",
    "\n",
    "def vectorize_description_scaled(text):\n",
    "    word_counts = Counter(text.split())\n",
    "    # Create a vector where each element is the frequency of a word in the basis_of_words\n",
    "    vector = [word_counts[word] if word in word_counts else 0 for word in top_words]\n",
    "    return vector\n",
    "\n",
    "#Groupby departments to get df of departments and all associated course descriptions\n",
    "concatenated_descriptions = combined_df.groupby('department')['description_x'].apply(' '.join).reset_index()\n",
    "concatenated_descriptions['cleaned_description'] = concatenated_descriptions['description_x'].apply(clean_description)\n",
    "concatenated_descriptions['cleaned_description'] = concatenated_descriptions['cleaned_description'].str.lower()\n",
    "\n",
    "all_words = ' '.join(concatenated_descriptions['cleaned_description']).split()\n",
    "word_freq = Counter(all_words)\n",
    "top_words = [word for word, freq in word_freq.most_common(500)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bigrams(doc):\n",
    "    bigrams = []\n",
    "    for token1, token2 in zip(doc[:-1], doc[1:]):\n",
    "        if not (token1.is_stop or token2.is_stop or token1.is_punct or token1.text == ',' or token2.is_punct or token2.text == ','): \n",
    "            bigrams.append((token1.text, token2.text))\n",
    "    return bigrams\n",
    "\n",
    "bigrams_list = []\n",
    "\n",
    "for description in concatenated_descriptions['description_x']:\n",
    "    doc = nlp(description)\n",
    "    bigrams = extract_bigrams(doc)\n",
    "    bigrams_list.extend(bigrams)\n",
    "\n",
    "# Count the frequency of bigrams\n",
    "bigram_counts = Counter(bigrams_list)\n",
    "most_common_bigrams = bigram_counts.most_common(500)\n",
    "\n",
    "# Count the frequency of single words\n",
    "all_words = ' '.join(combined_df['cleaned_description']).split()\n",
    "word_freq = Counter(all_words)\n",
    "all_features = [word for word, freq in word_freq.most_common(500)]\n",
    "\n",
    "# Combine words and bigrams\n",
    "all_features.extend(['_'.join(bigram) for bigram, count in most_common_bigrams])\n",
    "\n",
    "feature_dict = {word: idx for idx, word in enumerate(all_features)}\n",
    "\n",
    "def vectorize_input(input_string, feature_dict):\n",
    "    vector = [0] * len(feature_dict)\n",
    "    words = input_string.split()\n",
    "    bigrams = zip(words, words[1:])\n",
    "\n",
    "    for word in words:\n",
    "        if word in feature_dict:\n",
    "            vector[feature_dict[word]] += 1\n",
    "\n",
    "    for bigram in bigrams:\n",
    "        bigram = '_'.join(bigram)\n",
    "        if bigram in feature_dict:\n",
    "            vector[feature_dict[bigram]] += 1\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 1., 2., 2., 1., 1., 2., 0., 1., 1., 2., 1., 1., 1., 0., 2.,\n",
       "       1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize each course description\n",
    "def tokenize(description):\n",
    "    return description.lower().split()\n",
    "\n",
    "# Count occurrences of each word in each course description\n",
    "word_occurrences = {word: [] for word in all_features}\n",
    "\n",
    "for description in concatenated_descriptions['cleaned_description']:\n",
    "    tokens = tokenize(description)\n",
    "    counts = Counter(tokens)\n",
    "    for word in all_features:\n",
    "        word_occurrences[word].append(counts.get(word, 0))\n",
    "\n",
    "# Calculate the median occurrences for each word\n",
    "median_occurrences = {word: np.median(counts) for word, counts in word_occurrences.items()}\n",
    "\n",
    "# Create an array of median occurrences\n",
    "median_occurrences_array = np.array([median_occurrences[word] for word in all_features])\n",
    "\n",
    "median_occurrences_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('course', 'explores'), 167),\n",
       " (('course', 'examines'), 128),\n",
       " (('United', 'States'), 122),\n",
       " (('course', 'provides'), 106),\n",
       " (('course', 'focuses'), 105),\n",
       " (('Topics', 'include'), 85),\n",
       " ((' ', 'Students'), 81),\n",
       " (('course', 'introduces'), 74),\n",
       " (('introduce', 'students'), 70),\n",
       " (('course', 'covers'), 70),\n",
       " (('course', 'offers'), 69),\n",
       " (('case', 'studies'), 69),\n",
       " (('introduces', 'students'), 67),\n",
       " (('Middle', 'East'), 66),\n",
       " (('20th', 'century'), 61),\n",
       " (('language', 'skills'), 60),\n",
       " (('faculty', 'member'), 58),\n",
       " (('wide', 'range'), 57),\n",
       " (('Southeast', 'Asia'), 53),\n",
       " (('course', 'aims'), 46),\n",
       " (('nineteenth', 'century'), 44),\n",
       " (('Latin', 'America'), 40),\n",
       " (('help', 'students'), 39),\n",
       " (('climate', 'change'), 38),\n",
       " (('Native', 'American'), 38),\n",
       " (('twentieth', 'century'), 36),\n",
       " (('Latin', 'American'), 36),\n",
       " (('popular', 'culture'), 35),\n",
       " ((' ', 'Topics'), 35),\n",
       " (('provide', 'students'), 34),\n",
       " (('African', 'American'), 34),\n",
       " (('Mathematics', 'webpage'), 34),\n",
       " (('writing', 'skills'), 33),\n",
       " (('human', 'rights'), 33),\n",
       " (('19th', 'century'), 32),\n",
       " (('present', 'day'), 32),\n",
       " (('covered', 'include'), 31),\n",
       " (('North', 'America'), 29),\n",
       " (('mass', 'media'), 29),\n",
       " (('social', 'movements'), 28),\n",
       " (('course', 'considers'), 28),\n",
       " (('modern', 'world'), 28),\n",
       " ((' ', 'Emphasis'), 27),\n",
       " (('short', 'stories'), 27),\n",
       " (('literary', 'texts'), 27),\n",
       " (('social', 'change'), 26),\n",
       " (('political', 'science'), 26),\n",
       " (('civil', 'rights'), 26),\n",
       " (('years', 'ago'), 26),\n",
       " (('South', 'Asia'), 26),\n",
       " (('visual', 'art'), 25),\n",
       " (('Students', 'learn'), 25),\n",
       " (('American', 'history'), 25),\n",
       " (('everyday', 'life'), 25),\n",
       " (('national', 'identity'), 25),\n",
       " (('introductory', 'course'), 25),\n",
       " (('data', 'analysis'), 25),\n",
       " (('American', 'Studies'), 24),\n",
       " (('research', 'project'), 24),\n",
       " (('American', 'literature'), 24),\n",
       " (('grammatical', 'structures'), 24),\n",
       " (('Global', 'Change'), 24),\n",
       " (('second', 'half'), 23),\n",
       " (('wide', 'variety'), 23),\n",
       " (('Topics', 'covered'), 23),\n",
       " (('webpage', 'Background'), 23),\n",
       " (('World', 'War'), 22),\n",
       " (('research', 'projects'), 22),\n",
       " (('particular', 'attention'), 22),\n",
       " (('Students', 'work'), 22),\n",
       " (('critical', 'thinking'), 22),\n",
       " (('laboratory', 'course'), 22),\n",
       " (('Middle', 'Ages'), 22),\n",
       " (('health', 'care'), 21),\n",
       " (('special', 'attention'), 21),\n",
       " (('Hebrew', 'Bible'), 21),\n",
       " (('interdisciplinary', 'course'), 20),\n",
       " (('visual', 'culture'), 20),\n",
       " (('primary', 'source'), 20),\n",
       " (('political', 'economy'), 20),\n",
       " (('American', 'women'), 20),\n",
       " (('lecture', 'course'), 20),\n",
       " (('survey', 'course'), 20),\n",
       " (('high', 'school'), 20),\n",
       " (('field', 'trips'), 20),\n",
       " (('21st', 'century'), 19),\n",
       " (('students', 'interested'), 19),\n",
       " (('religious', 'traditions'), 19),\n",
       " (('research', 'methods'), 19),\n",
       " (('Central', 'Europe'), 19),\n",
       " (('graduate', 'students'), 19),\n",
       " (('Honors', 'thesis'), 19),\n",
       " (('language', 'use'), 19),\n",
       " (('early', 'modern'), 19),\n",
       " (('provides', 'students'), 19),\n",
       " (('language', 'sequence'), 19),\n",
       " (('digital', 'media'), 18),\n",
       " (('writing', 'assignments'), 18),\n",
       " (('social', 'sciences'), 18),\n",
       " (('cultural', 'context'), 18),\n",
       " (('particular', 'emphasis'), 18),\n",
       " (('based', 'course'), 18),\n",
       " (('social', 'science'), 18),\n",
       " (('final', 'exam'), 18),\n",
       " (('environmental', 'change'), 18),\n",
       " (('Students', 'enroll'), 18),\n",
       " (('course', 'surveys'), 17),\n",
       " (('basic', 'concepts'), 17),\n",
       " (('course', 'uses'), 17),\n",
       " (('environmental', 'issues'), 17),\n",
       " (('Civil', 'War'), 17),\n",
       " (('social', 'justice'), 17),\n",
       " (('communication', 'skills'), 17),\n",
       " (('visual', 'arts'), 17),\n",
       " (('course', 'deals'), 17),\n",
       " (('subject', 'matter'), 17),\n",
       " (('course', 'begins'), 17),\n",
       " (('Special', 'attention'), 17),\n",
       " (('continuing', 'proficiency'), 17),\n",
       " (('ancient', 'Greek'), 17),\n",
       " (('de', 'la'), 17),\n",
       " (('public', 'health'), 16),\n",
       " (('students', 'learn'), 16),\n",
       " (('native', 'speakers'), 16),\n",
       " (('Civil', 'Rights'), 16),\n",
       " (('public', 'policy'), 16),\n",
       " (('East', 'Asia'), 16),\n",
       " (('topics', 'include'), 16),\n",
       " (('material', 'culture'), 16),\n",
       " (('major', 'themes'), 16),\n",
       " (('cultural', 'forms'), 16),\n",
       " (('primary', 'sources'), 16),\n",
       " (('Cold', 'War'), 16),\n",
       " (('independent', 'study'), 16),\n",
       " (('ancient', 'Egyptian'), 16),\n",
       " (('East', 'Central'), 16),\n",
       " (('North', 'Africa'), 16),\n",
       " (('second', 'language'), 16),\n",
       " (('course', 'designed'), 16),\n",
       " (('problem', 'solving'), 16),\n",
       " (('Change', 'II'), 16),\n",
       " (('course', 'students'), 15),\n",
       " (('oral', 'presentations'), 15),\n",
       " (('American', 'culture'), 15),\n",
       " (('literary', 'works'), 15),\n",
       " (('creative', 'writing'), 15),\n",
       " (('broad', 'range'), 15),\n",
       " (('differential', 'equations'), 15),\n",
       " (('human', 'health'), 15),\n",
       " (('Arab', 'American'), 15),\n",
       " (('American', 'identity'), 15),\n",
       " (('Native', 'peoples'), 15),\n",
       " (('Concentration', 'Distributions'), 15),\n",
       " (('cultural', 'practices'), 15),\n",
       " (('questions', 'concerning'), 15),\n",
       " (('video', 'games'), 15),\n",
       " (('senior', 'Honors'), 15),\n",
       " (('course', 'emphasizes'), 15),\n",
       " (('different', 'ways'), 15),\n",
       " (('modern', 'period'), 15),\n",
       " (('late', '18th'), 15),\n",
       " (('18th', 'century'), 15),\n",
       " (('mathematical', 'models'), 15),\n",
       " (('linear', 'algebra'), 15),\n",
       " (('political', 'theory'), 15),\n",
       " (('social', 'media'), 14),\n",
       " (('American', 'society'), 14),\n",
       " (('feature', 'films'), 14),\n",
       " (('new', 'forms'), 14),\n",
       " (('historical', 'contexts'), 14),\n",
       " (('social', 'identity'), 14),\n",
       " (('case', 'study'), 14),\n",
       " (('course', 'include'), 14),\n",
       " (('course', 'investigates'), 14),\n",
       " (('Pacific', 'Islander'), 14),\n",
       " (('gender', 'relations'), 14),\n",
       " (('term', 'sequence'), 14),\n",
       " (('intermediate', 'level'), 14),\n",
       " (('Japanese', 'culture'), 14),\n",
       " (('Special', 'Topics'), 14),\n",
       " (('French', 'Revolution'), 14),\n",
       " (('Judaic', 'Studies'), 14),\n",
       " (('current', 'events'), 13),\n",
       " (('colonial', 'rule'), 13),\n",
       " (('small', 'groups'), 13),\n",
       " (('primary', 'focus'), 13),\n",
       " (('semester', 'sequence'), 13),\n",
       " (('social', 'groups'), 13),\n",
       " (('foreign', 'policy'), 13),\n",
       " (('long', 'history'), 13),\n",
       " (('ethnic', 'groups'), 13),\n",
       " (('D.', 'Europe'), 13),\n",
       " (('course', 'serves'), 13),\n",
       " (('Ann', 'Arbor'), 13),\n",
       " (('raises', 'questions'), 13),\n",
       " (('mass', 'culture'), 13),\n",
       " (('newspaper', 'articles'), 13),\n",
       " (('Great', 'Lakes'), 13),\n",
       " (('Central', 'Asia'), 13),\n",
       " (('basic', 'understanding'), 13),\n",
       " (('course', 'includes'), 13),\n",
       " (('current', 'debates'), 13),\n",
       " (('video', 'clips'), 13),\n",
       " (('second', 'year'), 13),\n",
       " ((' ', 'Course'), 13),\n",
       " (('daily', 'lives'), 13),\n",
       " (('environmental', 'problems'), 13),\n",
       " (('undergraduate', 'students'), 13),\n",
       " (('process', 'control'), 13),\n",
       " (('academic', 'term'), 13),\n",
       " (('communicative', 'skills'), 12),\n",
       " (('American', 'life'), 12),\n",
       " (('historical', 'perspective'), 12),\n",
       " (('South', 'Africa'), 12),\n",
       " (('social', 'issues'), 12),\n",
       " (('American', 'communities'), 12),\n",
       " (('Black', 'women'), 12),\n",
       " (('historical', 'development'), 12),\n",
       " (('social', 'organization'), 12),\n",
       " (('20th', 'centuries'), 12),\n",
       " (('Native', 'Americans'), 12),\n",
       " (('honors', 'thesis'), 12),\n",
       " (('human', 'beings'), 12),\n",
       " (('selected', 'topics'), 12),\n",
       " (('Honors', 'program'), 12),\n",
       " (('course', 'sequence'), 12),\n",
       " (('enable', 'students'), 12),\n",
       " (('students', 'develop'), 12),\n",
       " (('special', 'focus'), 12),\n",
       " (('natural', 'world'), 12),\n",
       " (('basic', 'grammar'), 12),\n",
       " (('previously', 'acquired'), 12),\n",
       " (('acquired', 'language'), 12),\n",
       " (('culturally', 'authentic'), 12),\n",
       " (('advanced', 'level'), 12),\n",
       " (('class', 'discussions'), 12),\n",
       " (('black', 'holes'), 12),\n",
       " (('Selected', 'topics'), 12),\n",
       " (('science', 'fiction'), 12),\n",
       " (('listening', 'skills'), 12),\n",
       " (('year', 'Yiddish'), 12),\n",
       " (('Yiddish', 'language'), 12),\n",
       " (('tools', 'necessary'), 12),\n",
       " (('Classical', 'Hebrew'), 12),\n",
       " (('biblical', 'text'), 12),\n",
       " (('historical', 'context'), 11),\n",
       " (('gender', 'identities'), 11),\n",
       " (('issues', 'related'), 11),\n",
       " (('cultural', 'heritage'), 11),\n",
       " (('fundamental', 'principles'), 11),\n",
       " (('students', 'enrolled'), 11),\n",
       " (('senior', 'year'), 11),\n",
       " (('Topics', 'vary'), 11),\n",
       " (('American', 'Indian'), 11),\n",
       " (('political', 'history'), 11),\n",
       " (('ancient', 'Greece'), 11),\n",
       " (('Supreme', 'Court'), 11),\n",
       " (('Southeast', 'Asian'), 11),\n",
       " (('cultural', 'products'), 11),\n",
       " (('different', 'kinds'), 11),\n",
       " (('archaeological', 'evidence'), 11),\n",
       " (('human', 'history'), 11),\n",
       " (('Honors', 'students'), 11),\n",
       " (('Near', 'East'), 11),\n",
       " (('source', 'materials'), 11),\n",
       " (('prior', 'knowledge'), 11),\n",
       " (('prepare', 'students'), 11),\n",
       " (('modern', 'era'), 11),\n",
       " (('Chinese', 'language'), 11),\n",
       " (('basic', 'skills'), 11),\n",
       " (('authentic', 'contexts'), 11),\n",
       " (('language', 'proficiency'), 11),\n",
       " (('basic', 'principles'), 11),\n",
       " (('Eastern', 'Europe'), 11),\n",
       " (('energy', 'balance'), 11),\n",
       " (('natural', 'environment'), 11),\n",
       " (('biological', 'processes'), 11),\n",
       " (('Second', 'Temple'), 11),\n",
       " (('Roman', 'Empire'), 11),\n",
       " (('Senior', 'Honors'), 11),\n",
       " (('public', 'opinion'), 11),\n",
       " (('statistical', 'methods'), 11),\n",
       " (('faculty', 'members'), 11),\n",
       " (('Spanish', 'language'), 11),\n",
       " (('Russian', 'literature'), 11),\n",
       " (('recent', 'years'), 10),\n",
       " (('better', 'understanding'), 10),\n",
       " (('communicative', 'activities'), 10),\n",
       " (('American', 'experience'), 10),\n",
       " (('new', 'media'), 10),\n",
       " (('lived', 'experience'), 10),\n",
       " (('course', 'traces'), 10),\n",
       " (('War', 'II'), 10),\n",
       " (('different', 'parts'), 10),\n",
       " (('African', 'writers'), 10),\n",
       " (('cultural', 'production'), 10),\n",
       " (('early', 'Christian'), 10),\n",
       " (('ethnic', 'identities'), 10),\n",
       " (('ethnic', 'difference'), 10),\n",
       " (('cultural', 'studies'), 10),\n",
       " (('historical', 'experience'), 10),\n",
       " (('natural', 'history'), 10),\n",
       " (('Early', 'Modern'), 10),\n",
       " (('Key', 'issues'), 10),\n",
       " (('economic', 'development'), 10),\n",
       " (('critically', 'evaluate'), 10),\n",
       " (('solving', 'problems'), 10),\n",
       " (('design', 'project'), 10),\n",
       " (('community', 'service'), 10),\n",
       " (('decision', 'making'), 10),\n",
       " (('Asian', 'American'), 10),\n",
       " (('discussed', 'include'), 10),\n",
       " (('sexual', 'orientation'), 10),\n",
       " (('cultural', 'history'), 10),\n",
       " (('Native', 'North'), 10),\n",
       " (('art', 'form'), 10),\n",
       " (('historical', 'texts'), 10),\n",
       " (('gender', 'roles'), 10),\n",
       " (('ancient', 'Egypt'), 10),\n",
       " (('new', 'ways'), 10),\n",
       " (('evolutionary', 'biology'), 10),\n",
       " (('cultural', 'processes'), 10),\n",
       " (('Japanese', 'language'), 10),\n",
       " (('close', 'reading'), 10),\n",
       " (('research', 'papers'), 10),\n",
       " (('expanding', 'vocabulary'), 10),\n",
       " (('authentic', 'materials'), 10),\n",
       " (('idiomatic', 'expressions'), 10),\n",
       " (('visual', 'materials'), 10),\n",
       " (('class', 'discussion'), 10),\n",
       " (('molecular', 'biology'), 10),\n",
       " (('students', 'intending'), 10),\n",
       " (('Israeli', 'conflict'), 10),\n",
       " (('natural', 'resource'), 10),\n",
       " (('themes', 'include'), 10),\n",
       " (('language', 'acquisition'), 10),\n",
       " (('different', 'areas'), 10),\n",
       " (('Soviet', 'Union'), 10),\n",
       " (('mechanical', 'engineering'), 10),\n",
       " (('level', 'course'), 9),\n",
       " (('skills', 'necessary'), 9),\n",
       " (('foreign', 'language'), 9),\n",
       " (('target', 'language'), 9),\n",
       " (('African', 'Diaspora'), 9),\n",
       " (('political', 'power'), 9),\n",
       " (('New', 'York'), 9),\n",
       " (('based', 'learning'), 9),\n",
       " (('deeper', 'understanding'), 9),\n",
       " (('rights', 'movement'), 9),\n",
       " (('course', 'addresses'), 9),\n",
       " (('course', 'looks'), 9),\n",
       " (('black', 'women'), 9),\n",
       " (('historical', 'circumstances'), 9),\n",
       " (('reading', 'list'), 9),\n",
       " (('real', 'life'), 9),\n",
       " (('earliest', 'times'), 9),\n",
       " (('paying', 'particular'), 9),\n",
       " (('final', 'paper'), 9),\n",
       " (('different', 'forms'), 9),\n",
       " (('language', 'contact'), 9),\n",
       " (('information', 'systems'), 9),\n",
       " (('numerical', 'methods'), 9),\n",
       " (('small', 'group'), 9),\n",
       " (('core', 'courses'), 9),\n",
       " (('human', 'relationships'), 9),\n",
       " (('Arab', 'Americans'), 9),\n",
       " (('Arab', 'world'), 9),\n",
       " (('Asian', 'Americans'), 9),\n",
       " (('course', 'content'), 9),\n",
       " (('cultural', 'contexts'), 9),\n",
       " (('topics', 'vary'), 9),\n",
       " (('political', 'battles'), 9),\n",
       " (('cultural', 'diversity'), 9),\n",
       " (('graphic', 'novels'), 9),\n",
       " (('cultural', 'construction'), 9),\n",
       " (('social', 'inequality'), 9),\n",
       " (('DNA', 'testing'), 9),\n",
       " (('contemporary', 'debates'), 9),\n",
       " (('research', 'skills'), 9),\n",
       " (('Jewish', 'culture'), 9),\n",
       " (('urban', 'planning'), 9),\n",
       " (('North', 'American'), 9),\n",
       " (('discussion', 'section'), 9),\n",
       " (('Honors', 'Program'), 9),\n",
       " (('cultural', 'anthropology'), 9),\n",
       " (('current', 'research'), 9),\n",
       " (('course', 'materials'), 9),\n",
       " (('Ottoman', 'Empire'), 9),\n",
       " (('better', 'understand'), 9),\n",
       " (('think', 'critically'), 9),\n",
       " (('major', 'issues'), 9),\n",
       " (('main', 'goal'), 9),\n",
       " (('Muslim', 'community'), 9),\n",
       " (('English', 'translation'), 9),\n",
       " (('social', 'history'), 9),\n",
       " (('human', 'language'), 9),\n",
       " (('Honors', 'Thesis'), 9),\n",
       " (('presenting', 'information'), 9),\n",
       " (('new', 'vocabulary'), 9),\n",
       " (('language', 'structures'), 9),\n",
       " (('listening', 'comprehension'), 9),\n",
       " (('course', 'number'), 9),\n",
       " (('linear', 'regression'), 9),\n",
       " (('biological', 'systems'), 9),\n",
       " (('Mediterranean', 'world'), 9),\n",
       " (('ancient', 'texts'), 9),\n",
       " (('centuries', 'B.C.'), 9),\n",
       " (('ancient', 'world'), 9),\n",
       " (('human', 'population'), 9),\n",
       " ((' ', 'Human'), 9),\n",
       " (('research', 'problems'), 9),\n",
       " (('real', 'world'), 9),\n",
       " (('Particular', 'attention'), 9),\n",
       " (('individual', 'research'), 9),\n",
       " (('intensive', 'study'), 9),\n",
       " (('fossil', 'fuels'), 9),\n",
       " (('folk', 'tales'), 9),\n",
       " (('team', 'sports'), 9),\n",
       " (('formative', 'period'), 9),\n",
       " (('revolutionary', 'movements'), 9),\n",
       " (('section', 'title'), 9),\n",
       " (('Islamic', 'law'), 9),\n",
       " (('speech', 'sounds'), 9),\n",
       " (('heat', 'transfer'), 9),\n",
       " (('cognitive', 'psychology'), 9),\n",
       " (('Puerto', 'Rican'), 8),\n",
       " (('oral', 'history'), 8),\n",
       " (('border', 'crossing'), 8),\n",
       " (('urban', 'life'), 8),\n",
       " (('strong', 'emphasis'), 8),\n",
       " (('popular', 'music'), 8),\n",
       " (('feminist', 'theory'), 8),\n",
       " (('reading', 'assignments'), 8),\n",
       " (('course', 'seeks'), 8),\n",
       " (('natural', 'resources'), 8),\n",
       " (('economic', 'issues'), 8),\n",
       " (('allow', 'students'), 8),\n",
       " (('allows', 'students'), 8),\n",
       " (('expose', 'students'), 8),\n",
       " (('course', 'combines'), 8),\n",
       " (('cultural', 'identity'), 8),\n",
       " (('international', 'relations'), 8),\n",
       " (('interdisciplinary', 'approach'), 8),\n",
       " (('late', 'nineteenth'), 8),\n",
       " (('historical', 'events'), 8),\n",
       " (('guest', 'speakers'), 8),\n",
       " (('intensive', 'introduction'), 8),\n",
       " (('Jackson', 'Hole'), 8),\n",
       " (('global', 'politics'), 8),\n",
       " (('social', 'structures'), 8),\n",
       " (('current', 'moment'), 8),\n",
       " (('political', 'institutions'), 8),\n",
       " (('second', 'term'), 8),\n",
       " (('anthropological', 'perspective'), 8),\n",
       " (('Nile', 'Valley'), 8),\n",
       " (('Honors', 'advisor'), 8),\n",
       " (('human', 'life'), 8),\n",
       " (('academic', 'discipline'), 8),\n",
       " (('anthropological', 'approaches'), 8),\n",
       " (('spoken', 'language'), 8),\n",
       " (('largest', 'country'), 8),\n",
       " (('general', 'introduction'), 8),\n",
       " (('cultural', 'aspects'), 8),\n",
       " (('Italian', 'Renaissance'), 8),\n",
       " (('Indian', 'subcontinent'), 8),\n",
       " (('intellectual', 'life'), 8),\n",
       " (('major', 'cities'), 8),\n",
       " (('Asian', 'Studies'), 8),\n",
       " (('develop', 'skills'), 8),\n",
       " (('historical', 'sources'), 8),\n",
       " (('modern', 'Japanese'), 8),\n",
       " (('daily', 'life'), 8),\n",
       " (('Thai', 'language'), 8),\n",
       " (('cultural', 'awareness'), 8),\n",
       " (('based', 'activities'), 8),\n",
       " (('reading', 'comprehension'), 8),\n",
       " (('sequence', 'designed'), 8),\n",
       " (('population', 'growth'), 8),\n",
       " (('term', 'course'), 8),\n",
       " (('homework', 'assignments'), 8),\n",
       " (('complex', 'systems'), 8),\n",
       " (('chemical', 'engineering'), 8),\n",
       " (('greenhouse', 'effect'), 8),\n",
       " (('ozone', 'depletion'), 8),\n",
       " (('quantum', 'mechanics'), 8),\n",
       " (('prepares', 'students'), 8),\n",
       " (('Jewish', 'identity'), 8),\n",
       " (('Mediterranean', 'region'), 8),\n",
       " ((' ', 'Special'), 8),\n",
       " (('Human', 'Impacts'), 8),\n",
       " ((' ', 'Global'), 8),\n",
       " (('global', 'change'), 8),\n",
       " (('Resources', ' '), 8),\n",
       " (('Development', ' '), 8),\n",
       " (('ice', 'sheets'), 8),\n",
       " (('psychological', 'processes'), 8),\n",
       " (('Comparative', 'Literature'), 8),\n",
       " (('work', 'closely'), 8),\n",
       " (('special', 'topics'), 8),\n",
       " (('environmental', 'impacts'), 8)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate words\n",
    "concatenated_descriptions['vector'] = concatenated_descriptions['cleaned_description'].apply(vectorize_description_scaled)\n",
    "\n",
    "#Generate cosine similarity matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def course_cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Make similarity matrix\n",
    "matrix = [vec for vec in concatenated_descriptions['vector']]\n",
    "# Compute cosine similarities\n",
    "similarity_matrix = cosine_similarity(matrix)\n",
    "#Find similar courses\n",
    "deps = concatenated_descriptions['department'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in two course names, the name of the column that holds the names, the dataframe, and the similarity matrix\n",
    "# and returns their cosine similarity\n",
    "def get_similarity(name1, name2, type, df, matrix):\n",
    "    idx_1 = df[df[type] == name1].index\n",
    "    idx_1 = idx_1[0]\n",
    "    idx_2 = df[df[type] == name2].index\n",
    "    idx_2 = idx_2[0]\n",
    "    return matrix[idx_1, idx_2]\n",
    "\n",
    "# Gets the course's cleaned description that is used to vectorize it\n",
    "def get_clean_desc(name):\n",
    "    idx_1 = concatenated_descriptions[concatenated_descriptions['department'] == name].index\n",
    "    idx_1 = idx_1[0]\n",
    "    return concatenated_descriptions.iloc[idx_1]['cleaned_description']\n",
    "\n",
    "# Gets the course's regular description that is used to vectorize it\n",
    "def get_reg_desc(name):\n",
    "    idx_1 = combined_df[combined_df['course'] == name].index\n",
    "    idx_1 = idx_1[0]\n",
    "    return combined_df.iloc[idx_1]['description_x']\n",
    "\n",
    "## OTHER FUNCTIONS\n",
    "def common_words(str1, str2, n):\n",
    "    # Splitting the strings into words\n",
    "    words_str1 = str1.split()\n",
    "    words_str2 = str2.split()\n",
    "\n",
    "    # Creating a dictionary to count the occurrences of each word in both strings\n",
    "    word_count = {}\n",
    "    for word in words_str1:\n",
    "        if word in words_str2:\n",
    "            word_count[word] = min(words_str1.count(word), words_str2.count(word))\n",
    "\n",
    "    # Sorting the dictionary by the frequency of the words\n",
    "    sorted_word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_word_count[0:n]\n",
    "\n",
    "def get_related(name, df, matrix, n):\n",
    "    n=n+1\n",
    "    idx = df[df['department'] == name].index\n",
    "    sims = matrix[idx]\n",
    "    # Get indices of sorted elements\n",
    "    sorted_indices = np.argsort(sims)\n",
    "    # Get indices of top n largest elements\n",
    "    top_n_indices = sorted_indices[0][-n:][::-1]\n",
    "    top_n_deps = np.array(deps)[top_n_indices]\n",
    "    top_n_deps = np.delete(top_n_deps, np.where(top_n_deps == name))\n",
    "    return top_n_deps\n",
    "\n",
    "def get_threshold(matrix, max_pairs):\n",
    "    \"\"\"\n",
    "    Find the threshold for similarity scores in the matrix.\n",
    "\n",
    "    :param matrix: A 2D numpy array containing similarity scores between classes.\n",
    "    :param max_pairs: Maximum number of class pairs with similarity above the threshold.\n",
    "    :return: Threshold value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Flatten the matrix to get all similarity scores\n",
    "    scores = matrix[np.triu_indices_from(matrix, k=1)]  # k=1 to exclude the diagonal\n",
    "\n",
    "    # Create bins for the range [0, 1]\n",
    "    num_bins = 100\n",
    "    bins = np.linspace(0, 1, num_bins + 1)\n",
    "    \n",
    "    # Count the number of scores in each bin\n",
    "    counts, _ = np.histogram(scores, bins)\n",
    "\n",
    "    # Find the cumulative count\n",
    "    cumulative_counts = np.cumsum(counts[::-1])[::-1]\n",
    "\n",
    "    # Find the bin where the cumulative count drops below the max_pairs\n",
    "    threshold_index = np.where(cumulative_counts <= max_pairs)[0]\n",
    "\n",
    "    if len(threshold_index) == 0:\n",
    "        return 1  # Return 1 if no such threshold exists (i.e., all scores are too low)\n",
    "\n",
    "    # Get the lower edge of the bin as the threshold\n",
    "    threshold_bin = threshold_index[0]\n",
    "    threshold = bins[threshold_bin]\n",
    "\n",
    "    return threshold\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
